Stake holders ask -
	Who your existing cutomers are and how do you retain them, in this competetive market?
	How do you find new business opportunities?
	How do you analyze the existing and analytical data?
	How do you build a system which is highly available and scalable?

Data Science is the answer for all, combination of 
	Statistics,
	Mathematics,
	Visual Techniques, and
	Programming

Python has extensive libraries, built in modules providing easy access to system functionalities
which not only improves accessibility but also provides standardised solutions for everyday programming challanges.
	Apart from being open Source Ecosystem
	Python and most its incredible libraries are platform independent
	Flat learning curve 

Data Science Overview
	Running Definitions
		1. A powerful new approach to make discoveries from data.
		2. An automated way to analyze enormous amounts of data and extract information
		3. A new discipline that combines aspects of statistics, mathematics,
	   	programming and visualization to turn data into information i.e. gain insights.

	Components of Data Science
		When Domain Expertise and Scientific Methods are combined with Technology, we get Data Science.
		Mathematical and Statistical Models			Operating System
		Analysis						Python Language
		Scientific Tools and Methods		+		Application Design
		Hypothesis Testing					Data Processing Tools
									Library

		Domain Expertise and Scientific Methods	
			Collect data
			Explore data
			Analyze and
			Visualize it.
			Find Patterns by applying mathematical & statistical models.

		Data Analysis Types which can be
			Descriptive - study data set to decipher the details i.e. to explain what happened.
			Predictive - one creates a model based on existing information to predict the outcome and behaviour
			Prescriptive - one suggests actions for a given situation using the collected information

		Data Scientists use this technologies
			build an automated model, build and train machine learning models
			Manipulate data with technology
			Extract information from data
			Build data tools, applications and services

	Data Scientist, a day in DS's Life
	1. Ask the right question, or start with a business problem
	2. Data aquisition i.e. data collection
	3. Interpretation and Data Wrangling invloves
		Data Cleansing
		Data Manipulation
		Data Discovery
		Data Pattern Identification
	4. Create and train model for machine learning
	5. Mathematical Statistical model
		6. Data Visualization
		7. Data Report
		8. Data Products

	A Data Scientist asks the right questions to the stakeholders, for which they need domain expertise
	curiosity to learn and create concepts and ability to communicate questions effectively to domain experts
	Should think analytically, to understand the hidden patterns in a data structure
	They should wrangle the data, by removing the redundancies, and irrelevant data collected from various sources
	Apply statistical thinking and applying mathematical methods is an important trait that a data scientist should have.
	Data should be visualised with graphics and combined with proper story telling to communicate to the audience.	
	To become adept with these skills they should follow a distinct roadmap.

	In Data Analytics, the data is acquired from various sources and
	is then wrangled to ease its analysis. This is followed by data exploration and data modeling.
	The final stage is data visualization,
	where the data is presented and the patterns are identified.

	acquires data from various sources and data points,
	performs data wrangling that makes the data available for analysis,
	creates reports and plots for data visualization

	Flow of Data Analytics
		Data acquisition, wrangling, exploration, modeling, and visualization

	make projects with real world datasets from
	1. data.gov
	2. Nycopendata
	3. gapminder
	
	
	Big Data
	A huge collection of data stored on distributed systems/ machines popularly reffered to as Hadoop clusters.
	Data Science helps extract information from the Data and build information-driven enterprises.
	
	Big Data and usually 3vs but we will study 5vs of Big Data
		
	1. Volume - Enormous amount of data generated from various sources
	2. Velocity - Large amouts if data streaming in at great speeds which require quick data processing
	3. Variety - Different formats of data: Structred, Semi-structured, and Unstructered
	4. Verasity
	5. Value

	Structured Data
		RDBMS data, stored and retrieved easily and quickly through SQL
	Semi structured Data, ususally in forms of file
		json, xml, NoSQL database
	Unstructured Data
		Images, Text File, Videos

	Real life UseCase Example -
	Google Search Engine's influencing factors
		1. QueryVolumes - measured by Unique and verifiable users
			Keywords/phrase matches on the web based on that it builds a query volume.
		2. Geographical Locations
			Tag a query to a specific location that is generated from
		3. Scrubbing for inappropriate content
		The Search Engine's Autocomplete feature identifies unique and verifiable users
		who search for a particular keyword or phrase to build a Query Volume,
		It also helps identify the users' locations and tag them to the query,
		enabling it to be location-specific.

	HealthCare -
		1. Wearable devices, these devices have biometric censors and built in processors to gather data from your body.
		 	-> BIOMETRIC DATA TRANSFER
		2. IOT GATEWAY, this platform ideally collects thousands of data points
			-> Data Transfer to Servers
		3. ENTERPRISE INFRASTRUCTURE, ingested into the system for furthur processesing.
		4. Big Data Analytics, platfrom applies data models created by data scientists and
				Extracts the information that is relevant to you.
			-> Sends the information to
		5. Engagement Dashboard, walk, sleep cycle to Make informed decisions and
			reduce overall health care and insurance costs.
			would also help your doctor record your vitals and diagnose your issue
	Finance -
		1. Loan Applicant
			-> Loan Manager submits the application to loan manager portal.
		2. Loan Application Portal
			-> Data Transfers to servers
		3. Enterprise Infrastructure
			->
		4. Data Analytics
		5. Engagement Dashboard
			credit report, credit history, approved amount, risk, collateral
			-> Make Informed Decisions
		6. Approve or Deny the Loan.

	Public Sector -
		Shared data with public, such as Climate change, Disease control
		with government data available about the country people are encouraged to use that data to
		create their own digital products and services. eg data.gov


	Challanges faced by a Data Scientist
	1. Data Quality doesn't conform to the set standards, inconsistent, inaccurate, incomplete
		not in the desirable formats and with anamolies
	2. Data integration is a complex task
	3. Data is distributed to Hadoop Distributed file system of HDFS, from various sources to ingest
		process and analyze and visualize huge data sets. 
	4. Size of these Hadoop clusters can very from a few nodes to 1000 nodes, the challange is
		to perform analytic on these large datasets efficiently and effectively.

	Python to the rescue with its powerful set of libraries, functions, modules, packages and extensions -
	to Aquire - Scrapy
	to Wrangle - Pandas, dataframes
	to Explore - Matplotlib
	to Model - Scikit learn, numpy
	to Visualization - Bokeh, SciPy

		PYTHON 
	open source programming language, that lets you work quickly and integrate systems more effectively
	NUMPY is fundamental package for scientific computing,
	SCIPY is the core of scientific computing libraries, provides many user friendly and
		efficient designed numerical routines.
	MATPLOT LIB is a python 2D plotting library, produces publication quality figures in a variety of hard copy formats
		and interative environments across platforms.
	SCIKITLEARN is built on numpy, scipy, and matplotlib for Data Mining and Data Analysis
	PANDAS is a library providing easy to use data structures and data analysis tools for python.	
	
	All these libraries, modules and packages are open source and hence using them is convinient and easy.
	
	Features -
	Easy to Learn
	Open Source
	Big open souce community
	Efficient and multiplatform support
	Integrate well with enterprise apps and systems
	Great Vendor and product support
	Huge colloection of libraries, funtions and modules	 

	Big Data platforms and Processing frameworks for Python
	Data Scientist
		^
	Python
		^
	Big Data Processing Framework - HadoopMapReduce, Spark, Flink
		^
	Enterprise Big Data Platform - Cloudera, Hortonworks, MapReduce
		^
	Big Data
		

DATA ANALYTICS OVERVIEW
	Data by itself is just an information source. But unless you can understand it,
	you will not be able to use it effectively
	A bank statement doesnt show you the spending or saving pattern of customer
	but the moment you present data as a line chart, you can easdily spot the overall transaction pattern
	if you plot this for 3 different months, you can see the shift in regular spending pattern,
	alerting banks to instances of identity theft or fraud
	for ecommerce website to analyis buying patterns of users
	bank data is higly structured.

	Data Analytics is a combination of processes to extract information from datasets.
	101010 -> Domain Knowledge + Statistical Skills + Programming + Mathematics -> Decision Making Process

	1. Identify business problems - Ask questions to identify business problem
	2. Data Aquisitions - Collecting data related to business problem from the real world
	3. Data Wrangling - data tools for data cleansing and data manipulation
	4. Exploratory data analysis (EDA) - mathematical or graphical output to aid data analysis
		Analysis of data using quantitative techniques
		Analysis of data using graphical techniques
		Suggests admissible models that best fit the data
			Approach - 
				studies the data to recommend admissible models that best fit the data
			Focus -
				Its focus is on the data
				its structures, outliers and models suggested by the data
			Assumptions - 
				EDA techinques make minimal or no assumptions.
				They present and show all the underlying data without any data loss.
			EDA Techniques - 
				Quantitavtive
					Mathematical and statistical functions provide	
					numeric outputs for the inputted data.
				Graphical
					use statistical functions for graphical output 
	
		
	5. Data Exploration - discover data and identify pattern in data
	6. Conclusion or Prediction - by creating training models for machine learning
	7. Communication or data visualisations - present the analysis work

1.	Identify Business Problems - 
		who are my customers?
		why are my sales going down?
		how do I manage my inventory?
		why is my system not scaling up with increase in traffic and volume?
		
		Such business problems trigger the need to analyze data and find answers.

		
2. 	Data Acquisition - 

		COLLECTION OF DATA FROM VARIOUS SOURCES for ANALYSIS to answer the question raised in step 1
		Data acquisition is a process to collect data from various data sources such as RDBMS,
		No SQL databases, web server logs and also scrape the web through web APIs
	
		Data Scientist Expertise, Data Scientist must use Database skills, such as
		File Handling
		File Formats - ability to deal with them
		Web Scrapping
		Twitter, Facebook, LinkedIn and other social media and information sites provide streaming APIs.
		Server logs can be extracted from enterprises system server to analyze and optimise application performance
		
3. 	DATA WRANGLING and Exploration - most important data Analytic Process
		Data Cleansing - gets rid of unwanted elements present in the data
		Data Manipulation - TRANSFORM, MERGE, AGGRIGATE, GROUP BY and RESHAPE
					make it available for exploritary data analysis
		Data Exploration - Uses all the available data and presents it in either a numerical or graphical 
			Data Discovery
			Data Pattern 
		these are FED into appropriate machine learning models, leading directly to the prediction
		and conclusion phase

	
	Causes of Challenges in Data Wrangling phases:
		Data is neither in the expected format or consistent
		Unexpected Data Format
		Erroneous data , data contains lots of errors and unwanted values that needs to be cleansed
		Voluminous data to be manipulated
			Classifying data into Linear or Clustered
		Determining relationship between variables between Observation, Feature, and Response
		Data wrangling includes data transformation, merging, aggregation, group by operation, and reshaping.
	
	Model Selection -
	Based on overall data analysis process, to draw conclusions and make accurate predictions
	Should be accurate to avoid iterations
	Depends on Pattern Identification and Algorithms
	Depends on Hypothesis Building and Testing
	Leads to building mathematical statistical functions

4. 	Exploratory data analysis (EDA)
		Analysis of data using quantitative techniques
		Analysis of data using graphical techniques
		Suggests admissible models that best fit the data
			Approach - 
				studies the data to recommend admissible models that best fit the data
			Focus -
				Its focus is on the data
				its structures, outliers and models suggested by the data
			Assumptions - 
				EDA techinques make minimal or no assumptions.
				They present and show all the underlying data without any data loss.
			EDA Techniques - 
				Quantitavtive
					Mathematical and statistical functions provide	
					numeric outputs for the inputted data.
					Measurement of Central tendency,
						Mean(average), suitable for symmetric distributions
						Median(exact middle value), suitable for skewed distributions
						and for catching outliers in the dataset
						Mode(frequency).
					
					Measurement of Spread
						Variance, appx the mean of the square of the deviations
						standard deviation, square root of the variance
						interquartile range, between 75-25th % i.e. 50%
				
				Graphical
					use statistical functions for graphical output 
					Histogram
					 graphically summarize distribution of univartiate data set. It Shows
						- the center or location of data
						- the spread of data
						- the skewness of data
						- the presence of outliers
						- the presence of multiple modes in the data
					Scatterplot
						represents relationship between two variables.
						Are variable x and y related, linearly related or non-linearly related.
						Does change in variation of Y depend on X
						Are there outliers?
6. Hypothesis begins at Data Exploration stage but becomes more mature in the conclusion or prediction phase. 
	used to establish the relationship between dependent
	for example - Holiday season in the year increases traffic, and purchases on the website.
	and independent variables, values that are manipulated by researchers or scientist
	
	"Dependent variable values are presumed to change as a result of changes made in independent variable."

	Hypothesis building using FEATURE ENGINEERING
	Domain knowledge leads to hypothesis buiding using feature engineering.
		Feature engineering, Makes sense of data, Construct new features from raw data automatically
		construct new features from raw data manually. BASED ON domain expertise

	HYPOTHESIS building using a model
	1. Model Building
		Indentify the best input variables
		Evaluate the models capacity to forecast with these variables
	2. Model Evaluation
		Train and test the model for accuracy
		Optimize model accuracy, performance, and comparision with other model			
	3. Model Deployment
		Use the model for prediction
		Use the mdoel for compare actual outcome with expectations
	Take two samples from the whole population
		sample one u1 and sample u2, then their mean is calculated and
		calculating the difference between the two means is hypothesis testing.
	Two kind of hypothesis can be made initially -

	ALTERNATIVE hypothesis u1 != u2 proposed model outcome is accurate and matches the data.
	There is a difference between the means of s1 and s2
	
	NULL HYPOTHESIS logical opposite of the alternative hypothesis
	There is no difference between mean of S1 and S2			
	
	Begins by dividing the data set into training and testing
	training dataset is used to build new proposed model, ranges between 60-80% of the data
	makes use of the available features and responses of the data sample
	Test dataset used to test the model, acts as new unseen data. 20-40%

	Null Hypothesis is proven true - Proposed model does not predict better that the existing model.
	Alternative hypothesis is proven true - Proposed model predicts better than the existing model.

HYPOTHESIS testing
	inferrential statistical techinique
	that determines if a certain condition is true for the population
	In a cloth manufaturer the hypothesis is that every dress is flawless
	a study of each dress manufatured and nothing the defects
	
	DECISION			H0 is TRUE	H0 is false
	Failed to Reject Null		Correct		Type 2 Error
	Reject Null			Type 1 Error	Correct
	
	Type-1 aplha Error rejects Null Hypothesis when it is TRUE
	Type-2 beta FAILS to reject the null hypothhesis when it is false
	p value the probablity of observing extreme values 

step 1
	H0(u1=u2) NULL HYPOTHESIS
	H1(u1!=u2) ALTERNATIVE HYPOTHESIS
step 2
	Set a significant level for the population
step 3
	Collect Data from population
step 4
	calculate p value
	reject null hypothesis if p<alpha
	Fail to reject null hypothesis p>=alpha
for example -
	null hypothesis will be two medicines of different pharmacutical company are equally effective
	alternative they are not

	efficacy of medicine
	CONTINUOUS DATA - 	evaluate mean, median standard deviation or varience 
	take temperature of patients after every hour of giving the medicine reffered to as continuous data
	binomial data - evaluate the percentage general classification of data. ask population if Fever Recided? YES/NO
	poisson data - evaluate rate of occurence or frequency. how many times a month they use certain medicine

TYPES OF DATA
three types of variable in categorical data 
	Nominal Variables - variables with no logical ordereing, Variables are independent of each other
	Ordinal Variables - Values are in logical order, relative distance between value is not clear.
	Association indicates if two variables are associated or independent of each other	

CHI SQUARE TEST !!!

	  purchases
 	<$500 	>$500
MALE	.55	.45
FEMALE	.75	.25

	NULL HYPOTHESIS there is no association between gender and purchase FE EXPECTED FREQUENCY bivariate table expected
			the probablity does not change for 500 dollars or more whether female or male
	ALTERNATIVE HYPOTHESIS	 FO OBSERVED FREQUENCY 
			there is association between gender and purchase
			the probablity of purchase over 500 dollars is different for female and male
	if there is no association between gender and purchase
	observed frequency = expected frequency

7. Communication, process and results are presented to the stakeholders.
	Visual Graphs,
	Plotting Maps
	Reports
	Whitepaper reports
	Data Visualisation
		establishes trends, simplifies quantitative information through visuals
		shows the relationship between data points and variables
		indentifies trends, webtraffic patterns
	Plotting
		represent underlying data through graphics.
		x and y dependency or independency 

	DATA TYPES FOR PLOTTING
	1. Numerical Data
	i. Discrete data - Distinct or counted values, eg. number of employees in a company
	ii. Continuous data - values within a range that can be measured, weight of a newborn baby in kilograms
				the daily wind speed
		If the data is continuous use HISTOGRAM, LINE CHART, REGRESSION PLOT
	2. Categorical Data
	i.  Cluster or a Grouped values eg.
	Students can be divided into different groups based on height, tall, medium, and short
	ii. Ordinal data - values grouped according to ranks
	Strongly agree, agree, disagree, five point scale ranks.

		if the data is categorical Scatter plot, cluster map, heatmap
	
	3. Time series data - data measured in time blocks such date, month, year and time (hours, minutes and seconds)
	


Data Analytics is an iterative process at every step we have to check back with the question,
often to ensure that we are on the right track
Process result - question is answered or business problem is solved.

Skills and tools required at each step of the Data Analytics Process

1. Question or Business Problem
	Ability to ask appropriate questions
	Domain knowledge 
	Passion for Data
	Analytical Approach
2. Data Aquisition
	BeautifulSoup for webscrapin
	CSV or other file knowledge
	NumPy
	Pandas
	Database
3. Data Wrangling
	CSV or other file knowledge
	NumPy
	Pandas
	Database
	Scipy
4. Data Exploration
	NumPy	
	SciPy
	Pandas
	Matplotlib
5. Conclusions or Predictions
	Scikit-learn
	CSV
	Numpy
	Pandas
	Database
	Scipy
6. Communication or Data Visualisation
	Pandas
	Database
	Matplotlib
	PPT
	csv

STATISTICAL ANALYSIS OVERVIEW -

Stats is the study of the collection, analysis, interpretation, presentation and organisation of data
widely used to understand the complex problem of real world and simplify them to make well informed decisions


STATISTICS difference between statistical and non statistical analysis  i.e. inferential analysi

	STATTISTICAL ANALYSIS is:
	scientific
	based on numbers or statistical values
	useful in providing complete insight to the data
	NON STATISTICAL is
	based on very generic information
	exclusive of statistical or quantitative analysis

	tools available to analyze data
	- statistical principles
	- functions
	- algorithms
	what can we do using statistical tool
	- analyze the primary data
	- build statistical model
	- predict future outcome

Two major categories of statistical analysis and their differences
	DESCRIPTIVE ANALYSIS
	Record height of each and every person. 
	It provides a consise summary of data, for eg user visiting your website in a week and summarize data.
		
	INFERENTIAL ANALYSIS
	random sample is drawn from the population
	used to describe and make inferences about the population
	Infer the tallest, shortest and average height of the population.
	valuable when it is not convenient to examine each member of the population.
	example categorize height as tall medium and short Take a sample to study from the popualtion. 
	
Statistical analysis considerations	
	PURPOSE - clear and well defined
	QUESTIONS - prepare questionnaire in advance
	AUDIENCE - select population based on the purpose of your analysis
	SAMPLE - determine the sample that you want to draw from the population based on the purpose of you study 


Important terms used to describe data -
1. Statistics - are quantitative values calculated from the sample,
2. Parameters - are the characteristics of the population

Suppose we have -> x0, x1, x2, x3, .... xn
	we wanna know vital information like average, most occuring characteristics and so on.
	calculated using these formulas -
	Mean - 		u(population parameter)			xbar(sample statistics) 		xbar = 1/n sum(Xi)
		the average, typicall value present in the distribution	
	Variance - 	sigma_square(population parameter) 	S^2(sample statistics) 			S^2 = 1/n-1 * (sum(Xi-xbar))^2
		measures the sample variablity
	Std dev - 	sigma(population parameter) 		S(sample statistics)			S = root[1/n-1 * (sum(Xi-xbar))^2]
		explains how spread out the data is from the mean,
		greater standard deviation, greater the spread in the data

3. Search - find an unusual data, data that does not match the parameters
4. Inspect - Refers to studying the shape and spread of dataset
5. Characterize - determining Central Tendency of data
6. Conclusion - refers to preliminary or high level conclusions about the data

STATISTICAL ANLYSIS PROCESS
	
	STEP 1 Find the population of interset that suits the purpose of statistical analysis
	STEP 2 Draw a random sample that represents the characteristics population
	STEP 3 Compute sample statistics to describe the spread and shape of the dataset.
	STEP 4 Make inferences using the sample and calculations and apply it back to the population

DATA DISTRIBUTION
	collection of values arranged in order of their relative frequency and occurences
	Range - refers to maximum and minimum value of data
	Frequency - refers to number of occurences of a data value
	Central tendency - indicates data accumulation
				toward the middle of the distribution or to the end.
			Mean is the average
			Median is the 50th Percentile
			Mode is the most frequent value

PERCENTILE- a measure used in statistics indicating value below in which
		a given percentage of observation in a group of observation falls 
		
		THIRD QUARTILE - 75th percentile = 91-100
		SECOND QUARTILE - 50th percentile = 80-90
		FIRST QUARTILE - 25th percentile = 59-79

DISPERSION - variablity, scatter or spread of how streached or squeezed a distribution is
		RANGE - range refers to difference between maximum and minimum data values
		INTERQUARTILE RANGE - difference between 25th and 75th percentiles
		VARIANCE - refers to data values around the mean value
		STANDARD DEVIATION - square root of the variance to indicate how spread out the data is.

	
HISTOGRAM - karl pearson
	to construct a historgram we have to bin the range of values
	i.e. divide the entire range of values into series of intervals and then count how many values,
	fall into each interval.
	BINS are consequtive non overlapping intervals of a variable
	BINS are of equal size, the bars represent the bin
	Height of the bar represents the frequency of the values in the BIN
		helps assess the probablity distribution of a variable by depecting observation occuring in a certain range
		of values.

BELL CURVE - NORMAL DISTRIBUTION
	Its symmetric around the mean 
	Symmetric on both sides of the center
	Having equal mean, median, and mode values
	Denser in the center and less dense in the tails or sides
	MEAN and STANDARD DEVIATION also known as the gaussian curve

	PEAK where most of the observations occurs, within one standard deviation from the mean
	FLANKS beyond the peak, but one and two standard deviations from the mean
	TAIL far from peak considered beyond two deviation from the mean 5% or less data falls under this
	
	SKEWED -
		data distribution indicates the tendency of the data distribution to be more spread out one side
	LEFT SKEWED
		MEAN<MEADIAN
		Negatively skewed data
		Left tail contains large distributions
	RIGHT SKEWED
		MEAN>MEDIAN
		positively skewed
		Right tail contains large distributions	
KRUTOSIS 
	describes the shape of a probabilty distribution
	Mesures the tendency of data towards the center or towards the tail
	PLATYKURTIC is negative kurtosis
	MESOCURTIC normal distribution curve
	LEPTOKURTIC positive distribution curve

WHY ANACONDA?
	Open Source Python Distribution
	400+ popular Python Packages
	Enterprise ready data analytics platform
	Modern Data Science Analytics architecture
	Multiworkload data analytics - wrangling, while running a machine learning algorithm
	Supports Big Data environments such as Hadoop Spark and GPUs
	Interative visualisations governance, security and operational support

	conda create --name myenv
	eg: conda create --name datascience

PYTHON
	import sys
	print(sys.version)
		3.10.0 (tags/v3.10.0:b494f59, Oct  4 2021, 19:00:18) [MSC v.1929 64 bit (AMD64)]
	import platform
	platform.python_version()
	
	When variable is assigned a value, it refers to the values' memory location or address and not the value itself.\
	For example -
		x = 7
		then this 7 is assigned a reference address in the memory, say <address 1>
		when incremented
		x = x + 1
		the calculation is done and this new value is assigned new address in memory location.
		say, <address 2>
		later on the <address 1> value is garbage collected
		Python has an automated garbage collection.
		It has an algorithm to deallocate objects which are no longer needed